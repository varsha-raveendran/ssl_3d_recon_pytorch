{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1e59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb41fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaly/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/pytorch3d/renderer/opengl/__init__.py:16: UserWarning: Can't import EGL, not importing MeshRasterizerOpenGL. This might happen if your Python application imported OpenGL with a non-EGL backend before importing PyTorch3D, or if you don't have pyopengl installed as part of your Python distribution.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'02691156': 'airplane', '02933112': 'cabinet', '02958343': 'car', '03001627': 'chair', '03636649': 'lamp', '04256520': 'sofa', '04379243': 'table', '04530566': 'watercraft'}\n"
     ]
    }
   ],
   "source": [
    "from src.network_architecture.recon_model import ReconstructionNet\n",
    "from src.network_architecture.pose_net import PoseNet\n",
    "from src.training import train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45d8b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'iso': True,\n",
    "    'use_symmetry_loss': True,\n",
    "     \"use_pretrained\" : True,\n",
    "    'experiment_name' : 'chair_full',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'category' : '03001627',\n",
    "    'batch_size': 4,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 1,\n",
    "    'validate_every_n': 1,\n",
    "    'learning_rate_pose_net' : 0.0005,\n",
    "    'learning_rate_recon_net' : 0.0005,\n",
    "    'n_proj' : 5,\n",
    "    'lambda_ae' : 100.0,\n",
    "    'lambda_3d' : 10000.0,\n",
    "    'lambda_pose' : 1.0,\n",
    "    'lambda_ae_mask' : 1000,\n",
    "    'lambda_mask_fwd' : .00001,\n",
    "    'lambda_mask_bwd' : .00001,\n",
    "    'lambda_symm' : 1,\n",
    "    'lambda_mask_pose' : 1.0,\n",
    "    'lambda_ae_pose' : 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d4456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.shapenet import ShapeNet\n",
    "trainset = ShapeNet('test',config['category'], config['n_proj'])\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4e15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import save_pcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce84024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimising item:  8f248770aa0b4d41e6fe3612af521500\n",
      "**********************************************\n",
      "Step :  0\n",
      "Iteration :  0\n",
      "Loss :  39.89380260598767\n",
      "**********************************************\n",
      "Step :  1\n",
      "Iteration :  1\n",
      "Loss :  29.93663907874012\n",
      "**********************************************\n",
      "Step :  2\n",
      "Iteration :  2\n",
      "Loss :  26.79623275602724\n",
      "**********************************************\n",
      "Step :  3\n",
      "Iteration :  3\n",
      "Loss :  22.678211275317558\n",
      "**********************************************\n",
      "Step :  4\n",
      "Iteration :  4\n",
      "Loss :  21.340702525152384\n",
      "**********************************************\n",
      "Step :  5\n",
      "Iteration :  5\n",
      "Loss :  20.403746368572136\n",
      "**********************************************\n",
      "Step :  6\n",
      "Iteration :  6\n",
      "Loss :  19.212477920125302\n",
      "**********************************************\n",
      "Step :  7\n",
      "Iteration :  7\n",
      "Loss :  16.73451317003104\n",
      "**********************************************\n",
      "Step :  8\n",
      "Iteration :  8\n",
      "Loss :  15.061695433564113\n",
      "**********************************************\n",
      "Step :  9\n",
      "Iteration :  9\n",
      "Loss :  13.399482731245577\n",
      "**********************************************\n",
      "Step :  10\n",
      "Iteration :  10\n",
      "Loss :  12.555881679726204\n",
      "**********************************************\n",
      "Step :  11\n",
      "Iteration :  11\n",
      "Loss :  12.483420785277039\n",
      "**********************************************\n",
      "Step :  12\n",
      "Iteration :  12\n",
      "Loss :  12.2617599257283\n",
      "**********************************************\n",
      "Step :  13\n",
      "Iteration :  13\n",
      "Loss :  11.771902722380789\n",
      "**********************************************\n",
      "Step :  14\n",
      "Iteration :  14\n",
      "Loss :  11.157087923941631\n",
      "**********************************************\n",
      "Step :  15\n",
      "Iteration :  15\n",
      "Loss :  10.653610499580807\n",
      "**********************************************\n",
      "Step :  16\n",
      "Iteration :  16\n",
      "Loss :  10.245955745192141\n",
      "**********************************************\n",
      "Step :  17\n",
      "Iteration :  17\n",
      "Loss :  9.932603944342862\n",
      "**********************************************\n",
      "Step :  18\n",
      "Iteration :  18\n",
      "Loss :  9.69225194668706\n",
      "**********************************************\n",
      "Step :  19\n",
      "Iteration :  19\n",
      "Loss :  9.543289194207766\n",
      "**********************************************\n",
      "Step :  20\n",
      "Iteration :  20\n",
      "Loss :  9.345328487757218\n",
      "**********************************************\n",
      "Step :  21\n",
      "Iteration :  21\n",
      "Loss :  9.171073756468298\n",
      "**********************************************\n",
      "Step :  22\n",
      "Iteration :  22\n",
      "Loss :  9.005163162308746\n",
      "**********************************************\n",
      "Step :  23\n",
      "Iteration :  23\n",
      "Loss :  8.846916765467109\n",
      "**********************************************\n",
      "Step :  24\n",
      "Iteration :  24\n",
      "Loss :  8.76486972567405\n",
      "**********************************************\n",
      "Step :  25\n",
      "Iteration :  25\n",
      "Loss :  8.68192933422219\n",
      "**********************************************\n",
      "Step :  26\n",
      "Iteration :  26\n",
      "Loss :  8.605265781721702\n",
      "**********************************************\n",
      "Step :  27\n",
      "Iteration :  27\n",
      "Loss :  8.537526580259714\n",
      "**********************************************\n",
      "Step :  28\n",
      "Iteration :  28\n",
      "Loss :  8.472108154928446\n",
      "**********************************************\n",
      "Step :  29\n",
      "Iteration :  29\n",
      "Loss :  8.411957959870158\n",
      "**********************************************\n",
      "Step :  30\n",
      "Iteration :  30\n",
      "Loss :  8.347534119689493\n",
      "**********************************************\n",
      "Step :  31\n",
      "Iteration :  31\n",
      "Loss :  8.300099851037475\n",
      "**********************************************\n",
      "Step :  32\n",
      "Iteration :  32\n",
      "Loss :  8.255128459068638\n",
      "**********************************************\n",
      "Step :  33\n",
      "Iteration :  33\n",
      "Loss :  8.216214341796265\n",
      "**********************************************\n",
      "Step :  34\n",
      "Iteration :  34\n",
      "Loss :  8.180028636443797\n",
      "**********************************************\n",
      "Step :  35\n",
      "Iteration :  35\n",
      "Loss :  8.138245631789651\n",
      "**********************************************\n",
      "Step :  36\n",
      "Iteration :  36\n",
      "Loss :  8.109969759655208\n",
      "**********************************************\n",
      "Step :  37\n",
      "Iteration :  37\n",
      "Loss :  8.08693224741792\n",
      "**********************************************\n",
      "Step :  38\n",
      "Iteration :  38\n",
      "Loss :  8.069218358324385\n",
      "**********************************************\n",
      "Step :  39\n",
      "Iteration :  39\n",
      "Loss :  8.052043369615722\n",
      "**********************************************\n",
      "Step :  40\n",
      "Iteration :  40\n",
      "Loss :  8.034451634994394\n",
      "**********************************************\n",
      "Step :  41\n",
      "Iteration :  41\n",
      "Loss :  8.01840972223568\n",
      "**********************************************\n",
      "Step :  42\n",
      "Iteration :  42\n",
      "Loss :  8.002459515601728\n",
      "**********************************************\n",
      "Step :  43\n",
      "Iteration :  43\n",
      "Loss :  7.988104451238225\n",
      "**********************************************\n",
      "Step :  44\n",
      "Iteration :  44\n",
      "Loss :  7.974460159423965\n",
      "**********************************************\n",
      "Step :  45\n",
      "Iteration :  45\n",
      "Loss :  7.961931497990902\n",
      "**********************************************\n",
      "Step :  46\n",
      "Iteration :  46\n",
      "Loss :  7.9501340970685686\n",
      "**********************************************\n",
      "Step :  47\n",
      "Iteration :  47\n",
      "Loss :  7.94064301605009\n",
      "**********************************************\n",
      "Step :  48\n",
      "Iteration :  48\n",
      "Loss :  7.93215701078036\n",
      "**********************************************\n",
      "Step :  49\n",
      "Iteration :  49\n",
      "Loss :  7.9239195371473095\n",
      "**********************************************\n",
      "Step :  50\n",
      "Iteration :  50\n",
      "Loss :  7.916692517744293\n",
      "**********************************************\n",
      "Step :  51\n",
      "Iteration :  51\n",
      "Loss :  7.910710651585042\n",
      "**********************************************\n",
      "Step :  52\n",
      "Iteration :  52\n",
      "Loss :  7.905827910913117\n",
      "**********************************************\n",
      "Step :  53\n",
      "Iteration :  53\n",
      "Loss :  7.902057252496061\n",
      "**********************************************\n",
      "Step :  54\n",
      "Iteration :  54\n",
      "Loss :  7.8977460426701995\n",
      "**********************************************\n",
      "Step :  55\n",
      "Iteration :  55\n",
      "Loss :  7.894043330358919\n",
      "**********************************************\n",
      "Step :  56\n",
      "Iteration :  56\n",
      "Loss :  7.890231691325911\n",
      "**********************************************\n",
      "Step :  57\n",
      "Iteration :  57\n",
      "Loss :  7.8851775501699715\n",
      "**********************************************\n",
      "Step :  58\n",
      "Iteration :  58\n",
      "Loss :  7.881599897698106\n",
      "**********************************************\n",
      "Step :  59\n",
      "Iteration :  59\n",
      "Loss :  7.878277780844595\n",
      "**********************************************\n",
      "Step :  60\n",
      "Iteration :  60\n",
      "Loss :  7.876012710073807\n",
      "**********************************************\n",
      "Step :  61\n",
      "Iteration :  61\n",
      "Loss :  7.873901920612596\n",
      "**********************************************\n",
      "Step :  62\n",
      "Iteration :  62\n",
      "Loss :  7.8712874369786885\n",
      "**********************************************\n",
      "Step :  63\n",
      "Iteration :  63\n",
      "Loss :  7.870009900245945\n",
      "**********************************************\n",
      "Step :  64\n",
      "Iteration :  64\n",
      "Loss :  7.867401993096103\n",
      "**********************************************\n",
      "Step :  65\n",
      "Iteration :  65\n",
      "Loss :  7.865867476253107\n",
      "**********************************************\n",
      "Step :  66\n",
      "Iteration :  66\n",
      "Loss :  7.864581163013311\n",
      "**********************************************\n",
      "Step :  67\n",
      "Iteration :  67\n",
      "Loss :  7.862991274094631\n",
      "**********************************************\n",
      "Step :  68\n",
      "Iteration :  68\n",
      "Loss :  7.861492989858732\n",
      "**********************************************\n",
      "Step :  69\n",
      "Iteration :  69\n",
      "Loss :  7.8600442463450495\n",
      "**********************************************\n",
      "Step :  70\n",
      "Iteration :  70\n",
      "Loss :  7.858753272720724\n",
      "**********************************************\n",
      "Step :  71\n",
      "Iteration :  71\n",
      "Loss :  7.857627627096505\n",
      "**********************************************\n",
      "Step :  72\n",
      "Iteration :  72\n",
      "Loss :  7.856574339227158\n",
      "**********************************************\n",
      "Step :  73\n",
      "Iteration :  73\n",
      "Loss :  7.855773013628864\n",
      "**********************************************\n",
      "Step :  74\n",
      "Iteration :  74\n",
      "Loss :  7.854991713717592\n",
      "**********************************************\n",
      "Step :  75\n",
      "Iteration :  75\n",
      "Loss :  7.854382334020862\n",
      "**********************************************\n",
      "Step :  76\n",
      "Iteration :  76\n",
      "Loss :  7.853253847281966\n",
      "**********************************************\n",
      "Step :  77\n",
      "Iteration :  77\n",
      "Loss :  7.852438282828538\n",
      "**********************************************\n",
      "Step :  78\n",
      "Iteration :  78\n",
      "Loss :  7.852011808766847\n",
      "**********************************************\n",
      "Step :  79\n",
      "Iteration :  79\n",
      "Loss :  7.851349088602364\n",
      "**********************************************\n",
      "Step :  80\n",
      "Iteration :  80\n",
      "Loss :  7.850717653286194\n",
      "**********************************************\n",
      "Step :  81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  81\n",
      "Loss :  7.8503180296521275\n",
      "**********************************************\n",
      "Step :  82\n",
      "Iteration :  82\n",
      "Loss :  7.849615933517823\n",
      "**********************************************\n",
      "Step :  83\n",
      "Iteration :  83\n",
      "Loss :  7.8499822308777745\n",
      "**********************************************\n",
      "Step :  84\n",
      "Iteration :  84\n",
      "Loss :  7.8487320507915115\n",
      "**********************************************\n",
      "Step :  85\n",
      "Iteration :  85\n",
      "Loss :  7.848422285994305\n",
      "**********************************************\n",
      "Step :  86\n",
      "Iteration :  86\n",
      "Loss :  7.847996325589246\n",
      "**********************************************\n",
      "Step :  87\n",
      "Iteration :  87\n",
      "Loss :  7.8477608067983375\n",
      "**********************************************\n",
      "Step :  88\n",
      "Iteration :  88\n",
      "Loss :  7.8473320730044875\n",
      "**********************************************\n",
      "Step :  89\n",
      "Iteration :  89\n",
      "Loss :  7.8470356341171446\n",
      "**********************************************\n",
      "Step :  90\n",
      "Iteration :  90\n",
      "Loss :  7.846678129644079\n",
      "**********************************************\n",
      "Step :  91\n",
      "Iteration :  91\n",
      "Loss :  7.846365300401855\n",
      "**********************************************\n",
      "Step :  92\n",
      "Iteration :  92\n",
      "Loss :  7.846152773953429\n",
      "**********************************************\n",
      "Step :  93\n",
      "Iteration :  93\n",
      "Loss :  7.8459480576266625\n",
      "**********************************************\n",
      "Step :  94\n",
      "Iteration :  94\n",
      "Loss :  7.845754672315661\n",
      "**********************************************\n",
      "Step :  95\n",
      "Iteration :  95\n",
      "Loss :  7.845433339604359\n",
      "**********************************************\n",
      "Step :  96\n",
      "Iteration :  96\n",
      "Loss :  7.845208927450168\n",
      "**********************************************\n",
      "Step :  97\n",
      "Iteration :  97\n",
      "Loss :  7.845080215773444\n",
      "**********************************************\n",
      "Step :  98\n",
      "Iteration :  98\n",
      "Loss :  7.844677430813448\n",
      "**********************************************\n",
      "Step :  99\n",
      "Iteration :  99\n",
      "Loss :  7.844504690514578\n",
      "**********************************************\n",
      "Step :  100\n",
      "Iteration :  100\n",
      "Loss :  7.844295090523699\n",
      "**********************************************\n",
      "Step :  101\n",
      "Iteration :  101\n",
      "Loss :  7.844913908980276\n",
      "**********************************************\n",
      "Step :  102\n",
      "Iteration :  102\n",
      "Loss :  7.84425122837523\n",
      "**********************************************\n",
      "Step :  103\n",
      "Iteration :  103\n",
      "Loss :  7.844282301163609\n",
      "**********************************************\n",
      "Step :  104\n",
      "Iteration :  104\n",
      "Loss :  7.843838537085147\n",
      "**********************************************\n",
      "Step :  105\n",
      "Iteration :  105\n",
      "Loss :  7.8439318663668836\n",
      "**********************************************\n",
      "Step :  106\n",
      "Iteration :  106\n",
      "Loss :  7.843600065583466\n",
      "**********************************************\n",
      "Step :  107\n",
      "Iteration :  107\n",
      "Loss :  7.843447788755256\n",
      "**********************************************\n",
      "Step :  108\n",
      "Iteration :  108\n",
      "Loss :  7.843547250593328\n",
      "**********************************************\n",
      "Step :  109\n",
      "Iteration :  109\n",
      "Loss :  7.843079604704116\n",
      "**********************************************\n",
      "Step :  110\n",
      "Iteration :  110\n",
      "Loss :  7.842845807303763\n",
      "**********************************************\n",
      "Step :  111\n",
      "Iteration :  111\n",
      "Loss :  7.843101432328565\n",
      "**********************************************\n",
      "Step :  112\n",
      "Iteration :  112\n",
      "Loss :  7.843098310642096\n",
      "**********************************************\n",
      "Step :  113\n",
      "Iteration :  113\n",
      "Loss :  7.842690248487255\n",
      "**********************************************\n",
      "Step :  114\n",
      "Iteration :  114\n",
      "Loss :  7.8429065816814045\n",
      "**********************************************\n",
      "Step :  115\n",
      "Iteration :  115\n",
      "Loss :  7.842765851174439\n",
      "**********************************************\n",
      "Step :  116\n",
      "Iteration :  116\n",
      "Loss :  7.842365055930241\n",
      "**********************************************\n",
      "Step :  117\n",
      "Iteration :  117\n",
      "Loss :  7.8427934870319085\n",
      "**********************************************\n",
      "Step :  118\n",
      "Iteration :  118\n",
      "Loss :  7.842510765949486\n",
      "**********************************************\n",
      "Step :  119\n",
      "Iteration :  119\n",
      "Loss :  7.842092297787352\n",
      "**********************************************\n",
      "Step :  120\n",
      "Iteration :  120\n",
      "Loss :  7.842520034346305\n",
      "**********************************************\n",
      "Step :  121\n",
      "Iteration :  121\n",
      "Loss :  7.842040633815275\n",
      "**********************************************\n",
      "Step :  122\n",
      "Iteration :  122\n",
      "Loss :  7.841799176517126\n",
      "**********************************************\n",
      "Step :  123\n",
      "Iteration :  123\n",
      "Loss :  7.842315344472828\n",
      "**********************************************\n",
      "Step :  124\n",
      "Iteration :  124\n",
      "Loss :  7.84171909425884\n",
      "**********************************************\n",
      "Step :  125\n",
      "Iteration :  125\n",
      "Loss :  7.841784224451587\n",
      "**********************************************\n",
      "Step :  126\n",
      "Iteration :  126\n",
      "Loss :  7.842083780949601\n",
      "**********************************************\n",
      "Step :  127\n",
      "Iteration :  127\n",
      "Loss :  7.8413998550138055\n",
      "**********************************************\n",
      "Step :  128\n",
      "Iteration :  128\n",
      "Loss :  7.841488541522615\n",
      "**********************************************\n",
      "Step :  129\n",
      "Iteration :  129\n",
      "Loss :  7.841522234883352\n",
      "**********************************************\n",
      "Step :  130\n",
      "Iteration :  130\n",
      "Loss :  7.841045278571027\n",
      "**********************************************\n",
      "Step :  131\n",
      "Iteration :  131\n",
      "Loss :  7.841436071130644\n",
      "**********************************************\n",
      "Step :  132\n",
      "Iteration :  132\n",
      "Loss :  7.841303209654141\n",
      "**********************************************\n",
      "Step :  133\n",
      "Iteration :  133\n",
      "Loss :  7.840904262347314\n",
      "**********************************************\n",
      "Step :  134\n",
      "Iteration :  134\n",
      "Loss :  7.841068794356371\n",
      "**********************************************\n",
      "Step :  135\n",
      "Iteration :  135\n",
      "Loss :  7.840845215084418\n",
      "**********************************************\n",
      "Step :  136\n",
      "Iteration :  136\n",
      "Loss :  7.840664953959088\n",
      "**********************************************\n",
      "Step :  137\n",
      "Iteration :  137\n",
      "Loss :  7.840742414650598\n",
      "**********************************************\n",
      "Step :  138\n",
      "Iteration :  138\n",
      "Loss :  7.840456286477684\n",
      "**********************************************\n",
      "Step :  139\n",
      "Iteration :  139\n",
      "Loss :  7.840739480853916\n",
      "**********************************************\n",
      "Step :  140\n",
      "Iteration :  140\n",
      "Loss :  7.840578354632214\n",
      "**********************************************\n",
      "Step :  141\n",
      "Iteration :  141\n",
      "Loss :  7.840377529867328\n",
      "**********************************************\n",
      "Step :  142\n",
      "Iteration :  142\n",
      "Loss :  7.8405594559321745\n",
      "**********************************************\n",
      "Step :  143\n",
      "Iteration :  143\n",
      "Loss :  7.840476266388924\n",
      "**********************************************\n",
      "Step :  144\n",
      "Iteration :  144\n",
      "Loss :  7.840182457509612\n",
      "**********************************************\n",
      "Step :  145\n",
      "Iteration :  145\n",
      "Loss :  7.8404039408976365\n",
      "**********************************************\n",
      "Step :  146\n",
      "Iteration :  146\n",
      "Loss :  7.840136066232802\n",
      "**********************************************\n",
      "Step :  147\n",
      "Iteration :  147\n",
      "Loss :  7.840088216039286\n",
      "**********************************************\n",
      "Step :  148\n",
      "Iteration :  148\n",
      "Loss :  7.840446498382068\n",
      "**********************************************\n",
      "Step :  149\n",
      "Iteration :  149\n",
      "Loss :  7.840086621293009\n",
      "**********************************************\n",
      "Step :  150\n",
      "Iteration :  150\n",
      "Loss :  7.839920968053924\n",
      "**********************************************\n",
      "Step :  151\n",
      "Iteration :  151\n",
      "Loss :  7.840120510086132\n",
      "**********************************************\n",
      "Step :  152\n",
      "Iteration :  152\n",
      "Loss :  7.8397117720519285\n",
      "**********************************************\n",
      "Step :  153\n",
      "Iteration :  153\n",
      "Loss :  7.839758399874489\n",
      "**********************************************\n",
      "Step :  154\n",
      "Iteration :  154\n",
      "Loss :  7.8397355015926475\n",
      "**********************************************\n",
      "Step :  155\n",
      "Iteration :  155\n",
      "Loss :  7.839730242677176\n",
      "**********************************************\n",
      "Step :  156\n",
      "Iteration :  156\n",
      "Loss :  7.839742095435513\n",
      "**********************************************\n",
      "Step :  157\n",
      "Iteration :  157\n",
      "Loss :  7.839725943856546\n",
      "**********************************************\n",
      "Step :  158\n",
      "Iteration :  158\n",
      "Loss :  7.839737102060477\n",
      "**********************************************\n",
      "Step :  159\n",
      "Iteration :  159\n",
      "Loss :  7.839661902477893\n",
      "**********************************************\n",
      "Step :  160\n",
      "Iteration :  160\n",
      "Loss :  7.8395649161418985\n",
      "**********************************************\n",
      "Step :  161\n",
      "Iteration :  161\n",
      "Loss :  7.839656404846009\n",
      "**********************************************\n",
      "Step :  162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  162\n",
      "Loss :  7.839676077641352\n",
      "**********************************************\n",
      "Step :  163\n",
      "Iteration :  163\n",
      "Loss :  7.839321560458245\n",
      "**********************************************\n",
      "Step :  164\n",
      "Iteration :  164\n",
      "Loss :  7.839403249284526\n",
      "**********************************************\n",
      "Step :  165\n",
      "Iteration :  165\n",
      "Loss :  7.83933448621695\n",
      "**********************************************\n",
      "Step :  166\n",
      "Iteration :  166\n",
      "Loss :  7.839324441036362\n",
      "**********************************************\n",
      "Step :  167\n",
      "Iteration :  167\n",
      "Loss :  7.839392928198445\n",
      "**********************************************\n",
      "Step :  168\n",
      "Iteration :  168\n",
      "Loss :  7.839317944881829\n",
      "**********************************************\n",
      "Step :  169\n",
      "Iteration :  169\n",
      "Loss :  7.839321176182688\n",
      "**********************************************\n",
      "Step :  170\n",
      "Iteration :  170\n",
      "Loss :  7.8392607173902995\n",
      "**********************************************\n",
      "Step :  171\n",
      "Iteration :  171\n",
      "Loss :  7.839134155060876\n",
      "**********************************************\n",
      "Step :  172\n",
      "Iteration :  172\n",
      "Loss :  7.8390534638128715\n",
      "**********************************************\n",
      "Step :  173\n",
      "Iteration :  173\n",
      "Loss :  7.839192896736078\n",
      "**********************************************\n",
      "Step :  174\n",
      "Iteration :  174\n",
      "Loss :  7.839112901403777\n",
      "**********************************************\n",
      "Step :  175\n",
      "Iteration :  175\n",
      "Loss :  7.83921752494098\n",
      "**********************************************\n",
      "Step :  176\n",
      "Iteration :  176\n",
      "Loss :  7.839131247011668\n",
      "**********************************************\n",
      "Step :  177\n",
      "Iteration :  177\n",
      "Loss :  7.839262033926184\n",
      "**********************************************\n",
      "Step :  178\n",
      "Iteration :  178\n",
      "Loss :  7.839124616866879\n",
      "**********************************************\n",
      "Step :  179\n",
      "Iteration :  179\n",
      "Loss :  7.839100211277756\n",
      "**********************************************\n",
      "Step :  180\n",
      "Iteration :  180\n",
      "Loss :  7.839067232021556\n",
      "**********************************************\n",
      "Step :  181\n",
      "Iteration :  181\n",
      "Loss :  7.839029572834184\n",
      "**********************************************\n",
      "Step :  182\n",
      "Iteration :  182\n",
      "Loss :  7.839001712157229\n",
      "**********************************************\n",
      "Step :  183\n",
      "Iteration :  183\n",
      "Loss :  7.838987701638901\n",
      "**********************************************\n",
      "Step :  184\n",
      "Iteration :  184\n",
      "Loss :  7.839052884331088\n",
      "**********************************************\n",
      "Step :  185\n",
      "Iteration :  185\n",
      "Loss :  7.838993574322677\n",
      "**********************************************\n",
      "Step :  186\n",
      "Iteration :  186\n",
      "Loss :  7.838778278947968\n",
      "**********************************************\n",
      "Step :  187\n",
      "Iteration :  187\n",
      "Loss :  7.839001506158015\n",
      "**********************************************\n",
      "Step :  188\n",
      "Iteration :  188\n",
      "Loss :  7.83913524398169\n",
      "**********************************************\n",
      "Step :  189\n",
      "Iteration :  189\n",
      "Loss :  7.839089655801432\n",
      "**********************************************\n",
      "Step :  190\n",
      "Iteration :  190\n",
      "Loss :  7.838824088671714\n",
      "**********************************************\n",
      "Step :  191\n",
      "Iteration :  191\n",
      "Loss :  7.838801895063523\n",
      "**********************************************\n",
      "Step :  192\n",
      "Iteration :  192\n",
      "Loss :  7.839183944990854\n",
      "**********************************************\n",
      "Step :  193\n",
      "Iteration :  193\n",
      "Loss :  7.839149958386983\n",
      "**********************************************\n",
      "Step :  194\n",
      "Iteration :  194\n",
      "Loss :  7.838961137607819\n",
      "**********************************************\n",
      "Step :  195\n",
      "Iteration :  195\n",
      "Loss :  7.838721113514359\n",
      "**********************************************\n",
      "Step :  196\n",
      "Iteration :  196\n",
      "Loss :  7.838848902218354\n",
      "**********************************************\n",
      "Step :  197\n",
      "Iteration :  197\n",
      "Loss :  7.839153899449043\n",
      "**********************************************\n",
      "Step :  198\n",
      "Iteration :  198\n",
      "Loss :  7.838879583857798\n",
      "**********************************************\n",
      "Step :  199\n",
      "Iteration :  199\n",
      "Loss :  7.838648904616241\n",
      "**********************************************\n",
      "Step :  200\n",
      "Iteration :  200\n",
      "Loss :  7.839036093329122\n",
      "**********************************************\n",
      "Step :  201\n",
      "Iteration :  201\n",
      "Loss :  7.838931023038752\n",
      "**********************************************\n",
      "Step :  202\n",
      "Iteration :  202\n",
      "Loss :  7.838973954983714\n",
      "**********************************************\n",
      "Step :  203\n",
      "Iteration :  203\n",
      "Loss :  7.838747998335613\n",
      "**********************************************\n",
      "Step :  204\n",
      "Iteration :  204\n",
      "Loss :  7.838794047829008\n",
      "**********************************************\n",
      "Step :  205\n",
      "Iteration :  205\n",
      "Loss :  7.838774624634007\n",
      "**********************************************\n",
      "Step :  206\n",
      "Iteration :  206\n",
      "Loss :  7.838730847541832\n",
      "**********************************************\n",
      "Step :  207\n",
      "Iteration :  207\n",
      "Loss :  7.838704595820851\n",
      "**********************************************\n",
      "Step :  208\n",
      "Iteration :  208\n",
      "Loss :  7.8387874427112365\n",
      "**********************************************\n",
      "Step :  209\n",
      "Iteration :  209\n",
      "Loss :  7.838815825841119\n",
      "**********************************************\n",
      "Step :  210\n",
      "Iteration :  210\n",
      "Loss :  7.8387931802267605\n",
      "**********************************************\n",
      "Step :  211\n",
      "Iteration :  211\n",
      "Loss :  7.838754229164085\n",
      "**********************************************\n",
      "Step :  212\n",
      "Iteration :  212\n",
      "Loss :  7.838521382458154\n",
      "**********************************************\n",
      "Step :  213\n",
      "Iteration :  213\n",
      "Loss :  7.8387309352296075\n",
      "**********************************************\n",
      "Step :  214\n",
      "Iteration :  214\n",
      "Loss :  7.838688367697298\n",
      "**********************************************\n",
      "Step :  215\n",
      "Iteration :  215\n",
      "Loss :  7.83888995662411\n",
      "**********************************************\n",
      "Step :  216\n",
      "Iteration :  216\n",
      "Loss :  7.838847386041253\n",
      "**********************************************\n",
      "Step :  217\n",
      "Iteration :  217\n",
      "Loss :  7.838766387369516\n",
      "**********************************************\n",
      "Step :  218\n",
      "Iteration :  218\n",
      "Loss :  7.838623942480302\n",
      "**********************************************\n",
      "Step :  219\n",
      "Iteration :  219\n",
      "Loss :  7.838757460600515\n",
      "**********************************************\n",
      "Step :  220\n",
      "Iteration :  220\n",
      "Loss :  7.839017659947257\n",
      "**********************************************\n",
      "Step :  221\n",
      "Iteration :  221\n",
      "Loss :  7.838889762846653\n",
      "**********************************************\n",
      "Step :  222\n",
      "Iteration :  222\n",
      "Loss :  7.838733093274108\n",
      "**********************************************\n",
      "Step :  223\n",
      "Iteration :  223\n",
      "Loss :  7.838651674285574\n",
      "**********************************************\n",
      "Step :  224\n",
      "Iteration :  224\n",
      "Loss :  7.838700934153032\n",
      "**********************************************\n",
      "Step :  225\n",
      "Iteration :  225\n",
      "Loss :  7.838779680078567\n",
      "**********************************************\n",
      "Step :  226\n",
      "Iteration :  226\n",
      "Loss :  7.8386334088206455\n",
      "**********************************************\n",
      "Step :  227\n",
      "Iteration :  227\n",
      "Loss :  7.838512892868984\n",
      "**********************************************\n",
      "Step :  228\n",
      "Iteration :  228\n",
      "Loss :  7.8386821938252\n",
      "**********************************************\n",
      "Step :  229\n",
      "Iteration :  229\n",
      "Loss :  7.838643731705072\n",
      "**********************************************\n",
      "Step :  230\n",
      "Iteration :  230\n",
      "Loss :  7.838806602466095\n",
      "**********************************************\n",
      "Step :  231\n",
      "Iteration :  231\n",
      "Loss :  7.838610913241893\n",
      "**********************************************\n",
      "Step :  232\n",
      "Iteration :  232\n",
      "Loss :  7.838468045598336\n",
      "**********************************************\n",
      "Step :  233\n",
      "Iteration :  233\n",
      "Loss :  7.838664527631601\n",
      "**********************************************\n",
      "Step :  234\n",
      "Iteration :  234\n",
      "Loss :  7.8386585843196706\n",
      "**********************************************\n",
      "Step :  235\n",
      "Iteration :  235\n",
      "Loss :  7.838747313400889\n",
      "**********************************************\n",
      "Step :  236\n",
      "Iteration :  236\n",
      "Loss :  7.838794458179628\n",
      "**********************************************\n",
      "Step :  237\n",
      "Iteration :  237\n",
      "Loss :  7.838662066474877\n",
      "**********************************************\n",
      "Step :  238\n",
      "Iteration :  238\n",
      "Loss :  7.838474910544155\n",
      "**********************************************\n",
      "Step :  239\n",
      "Iteration :  239\n",
      "Loss :  7.838679911262183\n",
      "**********************************************\n",
      "Step :  240\n",
      "Iteration :  240\n",
      "Loss :  7.83868730321608\n",
      "**********************************************\n",
      "Step :  241\n",
      "Iteration :  241\n",
      "Loss :  7.838764797953263\n",
      "**********************************************\n",
      "Step :  242\n",
      "Iteration :  242\n",
      "Loss :  7.838800314990161\n",
      "**********************************************\n",
      "Step :  243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  243\n",
      "Loss :  7.838658816278642\n",
      "**********************************************\n",
      "Step :  244\n",
      "Iteration :  244\n",
      "Loss :  7.838765134869494\n",
      "**********************************************\n",
      "Step :  245\n",
      "Iteration :  245\n",
      "Loss :  7.8387464981582795\n",
      "**********************************************\n",
      "Step :  246\n",
      "Iteration :  246\n",
      "Loss :  7.8386146559507175\n",
      "**********************************************\n",
      "Step :  247\n",
      "Iteration :  247\n",
      "Loss :  7.838657411324918\n",
      "**********************************************\n",
      "Step :  248\n",
      "Iteration :  248\n",
      "Loss :  7.838644382109626\n",
      "**********************************************\n",
      "Step :  249\n",
      "Iteration :  249\n",
      "Loss :  7.838775096107785\n",
      "**********************************************\n",
      "Step :  250\n",
      "Iteration :  250\n",
      "Loss :  7.838703493965525\n",
      "**********************************************\n",
      "Step :  251\n",
      "Iteration :  251\n",
      "Loss :  7.838574653280488\n",
      "**********************************************\n",
      "Step :  252\n",
      "Iteration :  252\n",
      "Loss :  7.838771416174489\n",
      "**********************************************\n",
      "Step :  253\n",
      "Iteration :  253\n",
      "Loss :  7.83872382355697\n",
      "**********************************************\n",
      "Step :  254\n",
      "Iteration :  254\n",
      "Loss :  7.8388600581911785\n",
      "**********************************************\n",
      "Step :  255\n",
      "Iteration :  255\n",
      "Loss :  7.8386905798221\n",
      "**********************************************\n",
      "Step :  256\n",
      "Iteration :  256\n",
      "Loss :  7.838639788489053\n",
      "**********************************************\n",
      "Step :  257\n",
      "Iteration :  257\n",
      "Loss :  7.838819775473889\n",
      "**********************************************\n",
      "Step :  258\n",
      "Iteration :  258\n",
      "Loss :  7.83893035965242\n",
      "**********************************************\n",
      "Step :  259\n",
      "Iteration :  259\n",
      "Loss :  7.838636969819211\n",
      "**********************************************\n",
      "Step :  260\n",
      "Iteration :  260\n",
      "Loss :  7.838708651334709\n",
      "**********************************************\n",
      "Step :  261\n",
      "Iteration :  261\n",
      "Loss :  7.838948202376056\n",
      "**********************************************\n",
      "Step :  262\n",
      "Iteration :  262\n",
      "Loss :  7.839004694282031\n",
      "**********************************************\n",
      "Step :  263\n",
      "Iteration :  263\n",
      "Loss :  7.838729504711809\n",
      "**********************************************\n",
      "Step :  264\n",
      "Iteration :  264\n",
      "Loss :  7.838559166876392\n",
      "**********************************************\n",
      "Step :  265\n",
      "Iteration :  265\n",
      "Loss :  7.83873465632436\n",
      "**********************************************\n",
      "Step :  266\n",
      "Iteration :  266\n",
      "Loss :  7.838708753323688\n",
      "**********************************************\n",
      "Step :  267\n",
      "Iteration :  267\n",
      "Loss :  7.838800208133088\n",
      "**********************************************\n",
      "Step :  268\n",
      "Iteration :  268\n",
      "Loss :  7.8385374961179135\n",
      "**********************************************\n",
      "Step :  269\n",
      "Iteration :  269\n",
      "Loss :  7.83859560086175\n",
      "**********************************************\n",
      "Step :  270\n",
      "Iteration :  270\n",
      "Loss :  7.8386249390985\n",
      "**********************************************\n",
      "Step :  271\n",
      "Iteration :  271\n",
      "Loss :  7.838759235370993\n",
      "**********************************************\n",
      "Step :  272\n",
      "Iteration :  272\n",
      "Loss :  7.838880631155698\n",
      "**********************************************\n",
      "Step :  273\n",
      "Iteration :  273\n",
      "Loss :  7.838570036448148\n",
      "**********************************************\n",
      "Step :  274\n",
      "Iteration :  274\n",
      "Loss :  7.838537830982116\n",
      "**********************************************\n",
      "Step :  275\n",
      "Iteration :  275\n",
      "Loss :  7.838482994069297\n",
      "**********************************************\n",
      "Step :  276\n",
      "Iteration :  276\n",
      "Loss :  7.8386763949105145\n",
      "**********************************************\n",
      "Step :  277\n",
      "Iteration :  277\n",
      "Loss :  7.8388042997668705\n",
      "**********************************************\n",
      "Step :  278\n",
      "Iteration :  278\n",
      "Loss :  7.8387096794718225\n",
      "**********************************************\n",
      "Step :  279\n",
      "Iteration :  279\n",
      "Loss :  7.838431602019448\n",
      "**********************************************\n",
      "Step :  280\n",
      "Iteration :  280\n",
      "Loss :  7.838579145065665\n",
      "**********************************************\n",
      "Step :  281\n",
      "Iteration :  281\n",
      "Loss :  7.838811000106627\n",
      "**********************************************\n",
      "Step :  282\n",
      "Iteration :  282\n",
      "Loss :  7.838972817739558\n",
      "**********************************************\n",
      "Step :  283\n",
      "Iteration :  283\n",
      "Loss :  7.838642464199628\n",
      "**********************************************\n",
      "Step :  284\n",
      "Iteration :  284\n",
      "Loss :  7.83858246201015\n",
      "**********************************************\n",
      "Step :  285\n",
      "Iteration :  285\n",
      "Loss :  7.83854929585965\n",
      "**********************************************\n",
      "Step :  286\n",
      "Iteration :  286\n",
      "Loss :  7.838797024969554\n",
      "**********************************************\n",
      "Step :  287\n",
      "Iteration :  287\n",
      "Loss :  7.838756242524433\n",
      "**********************************************\n",
      "Step :  288\n",
      "Iteration :  288\n",
      "Loss :  7.838580976495917\n",
      "**********************************************\n",
      "Step :  289\n",
      "Iteration :  289\n",
      "Loss :  7.838471817893151\n",
      "**********************************************\n",
      "Step :  290\n",
      "Iteration :  290\n",
      "Loss :  7.838577734532309\n",
      "**********************************************\n",
      "Step :  291\n",
      "Iteration :  291\n",
      "Loss :  7.838708174247667\n",
      "**********************************************\n",
      "Step :  292\n",
      "Iteration :  292\n",
      "Loss :  7.838882450118639\n",
      "**********************************************\n",
      "Step :  293\n",
      "Iteration :  293\n",
      "Loss :  7.839082332245954\n",
      "**********************************************\n",
      "Step :  294\n",
      "Iteration :  294\n",
      "Loss :  7.839616678154794\n",
      "**********************************************\n",
      "Step :  295\n",
      "Iteration :  295\n",
      "Loss :  7.839808681883117\n",
      "**********************************************\n",
      "Step :  296\n",
      "Iteration :  296\n",
      "Loss :  7.839548923723861\n",
      "**********************************************\n",
      "Step :  297\n",
      "Iteration :  297\n",
      "Loss :  7.83869146319992\n",
      "**********************************************\n",
      "Step :  298\n",
      "Iteration :  298\n",
      "Loss :  7.838641884878918\n",
      "**********************************************\n",
      "Step :  299\n",
      "Iteration :  299\n",
      "Loss :  7.839529858447204\n",
      "**********************************************\n",
      "Step :  300\n",
      "Iteration :  300\n",
      "Loss :  7.84002483341183\n",
      "**********************************************\n",
      "Step :  301\n",
      "Iteration :  301\n",
      "Loss :  7.8397183069971295\n",
      "**********************************************\n",
      "Step :  302\n",
      "Iteration :  302\n",
      "Loss :  7.838740585737066\n",
      "**********************************************\n",
      "Step :  303\n",
      "Iteration :  303\n",
      "Loss :  7.838584563199454\n",
      "**********************************************\n",
      "Step :  304\n",
      "Iteration :  304\n",
      "Loss :  7.839357576478237\n",
      "**********************************************\n",
      "Step :  305\n",
      "Iteration :  305\n",
      "Loss :  7.839555703505488\n",
      "**********************************************\n",
      "Step :  306\n",
      "Iteration :  306\n",
      "Loss :  7.839222336116869\n",
      "**********************************************\n",
      "Step :  307\n",
      "Iteration :  307\n",
      "Loss :  7.838579104411094\n",
      "**********************************************\n",
      "Step :  308\n",
      "Iteration :  308\n",
      "Loss :  7.838529052101154\n",
      "**********************************************\n",
      "Step :  309\n",
      "Iteration :  309\n",
      "Loss :  7.8390179286010415\n",
      "**********************************************\n",
      "Step :  310\n",
      "Iteration :  310\n",
      "Loss :  7.838819551755007\n",
      "**********************************************\n",
      "Step :  311\n",
      "Iteration :  311\n",
      "Loss :  7.838629226588853\n",
      "**********************************************\n",
      "Step :  312\n",
      "Iteration :  312\n",
      "Loss :  7.838797044372634\n",
      "**********************************************\n",
      "Step :  313\n",
      "Iteration :  313\n",
      "Loss :  7.838721944027345\n",
      "**********************************************\n",
      "Step :  314\n",
      "Iteration :  314\n",
      "Loss :  7.838830691740021\n",
      "**********************************************\n",
      "Step :  315\n",
      "Iteration :  315\n",
      "Loss :  7.8386640771070235\n",
      "**********************************************\n",
      "Step :  316\n",
      "Iteration :  316\n",
      "Loss :  7.83862500181173\n",
      "**********************************************\n",
      "Step :  317\n",
      "Iteration :  317\n",
      "Loss :  7.838760399038582\n",
      "**********************************************\n",
      "Step :  318\n",
      "Iteration :  318\n",
      "Loss :  7.838616733703708\n",
      "**********************************************\n",
      "Step :  319\n",
      "Iteration :  319\n",
      "Loss :  7.8389031494287185\n",
      "**********************************************\n",
      "Step :  320\n",
      "Iteration :  320\n",
      "Loss :  7.838592661268563\n",
      "**********************************************\n",
      "Step :  321\n",
      "Iteration :  321\n",
      "Loss :  7.838560561367268\n",
      "**********************************************\n",
      "Step :  322\n",
      "Iteration :  322\n",
      "Loss :  7.8386334174023435\n",
      "**********************************************\n",
      "Step :  323\n",
      "Iteration :  323\n",
      "Loss :  7.838646533121564\n",
      "**********************************************\n",
      "Step :  324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  324\n",
      "Loss :  7.8387265885541275\n",
      "**********************************************\n",
      "Step :  325\n",
      "Iteration :  325\n",
      "Loss :  7.83862023574274\n",
      "**********************************************\n",
      "Step :  326\n",
      "Iteration :  326\n",
      "Loss :  7.838652156827775\n",
      "**********************************************\n",
      "Step :  327\n",
      "Iteration :  327\n",
      "Loss :  7.838866261369002\n",
      "**********************************************\n",
      "Step :  328\n",
      "Iteration :  328\n",
      "Loss :  7.8387488398582565\n",
      "**********************************************\n",
      "Step :  329\n",
      "Iteration :  329\n",
      "Loss :  7.8387801749509265\n",
      "**********************************************\n",
      "Step :  330\n",
      "Iteration :  330\n",
      "Loss :  7.838569906894524\n",
      "**********************************************\n",
      "Step :  331\n",
      "Iteration :  331\n",
      "Loss :  7.8385880453688275\n",
      "**********************************************\n",
      "Step :  332\n",
      "Iteration :  332\n",
      "Loss :  7.838691492517365\n",
      "**********************************************\n",
      "Step :  333\n",
      "Iteration :  333\n",
      "Loss :  7.838803689126319\n",
      "**********************************************\n",
      "Step :  334\n",
      "Iteration :  334\n",
      "Loss :  7.838715980893153\n",
      "**********************************************\n",
      "Step :  335\n",
      "Iteration :  335\n",
      "Loss :  7.838549919493015\n",
      "**********************************************\n",
      "Step :  336\n",
      "Iteration :  336\n",
      "Loss :  7.838602010323068\n",
      "**********************************************\n",
      "Step :  337\n",
      "Iteration :  337\n",
      "Loss :  7.838956913803252\n",
      "**********************************************\n",
      "Step :  338\n",
      "Iteration :  338\n",
      "Loss :  7.83891618865011\n",
      "**********************************************\n",
      "Step :  339\n",
      "Iteration :  339\n",
      "Loss :  7.838893149260516\n",
      "**********************************************\n",
      "Step :  340\n",
      "Iteration :  340\n",
      "Loss :  7.838611190552958\n",
      "**********************************************\n",
      "Step :  341\n",
      "Iteration :  341\n",
      "Loss :  7.838545753234874\n",
      "**********************************************\n",
      "Step :  342\n",
      "Iteration :  342\n",
      "Loss :  7.8387716895248944\n",
      "**********************************************\n",
      "Step :  343\n",
      "Iteration :  343\n",
      "Loss :  7.838765700244383\n",
      "**********************************************\n",
      "Step :  344\n",
      "Iteration :  344\n",
      "Loss :  7.838671552707762\n",
      "**********************************************\n",
      "Step :  345\n",
      "Iteration :  345\n",
      "Loss :  7.838524018506123\n",
      "**********************************************\n",
      "Step :  346\n",
      "Iteration :  346\n",
      "Loss :  7.838598743155351\n",
      "**********************************************\n",
      "Step :  347\n",
      "Iteration :  347\n",
      "Loss :  7.838668509418655\n",
      "**********************************************\n",
      "Step :  348\n",
      "Iteration :  348\n",
      "Loss :  7.8387509448369785\n",
      "**********************************************\n",
      "Step :  349\n",
      "Iteration :  349\n",
      "Loss :  7.83865582919102\n",
      "**********************************************\n",
      "Step :  350\n",
      "Iteration :  350\n",
      "Loss :  7.8384778192176014\n",
      "**********************************************\n",
      "Step :  351\n",
      "Iteration :  351\n",
      "Loss :  7.838535888320846\n",
      "**********************************************\n",
      "Step :  352\n",
      "Iteration :  352\n",
      "Loss :  7.8387081988360094\n",
      "**********************************************\n",
      "Step :  353\n",
      "Iteration :  353\n",
      "Loss :  7.838831245047084\n",
      "**********************************************\n",
      "Step :  354\n",
      "Iteration :  354\n",
      "Loss :  7.838819300075756\n",
      "**********************************************\n",
      "Step :  355\n",
      "Iteration :  355\n",
      "Loss :  7.8386399751424465\n",
      "**********************************************\n",
      "Step :  356\n",
      "Iteration :  356\n",
      "Loss :  7.8385808494219695\n",
      "**********************************************\n",
      "Step :  357\n",
      "Iteration :  357\n",
      "Loss :  7.838669694811156\n",
      "**********************************************\n",
      "Step :  358\n",
      "Iteration :  358\n",
      "Loss :  7.838652241278798\n",
      "**********************************************\n",
      "Step :  359\n",
      "Iteration :  359\n",
      "Loss :  7.8386532783166185\n",
      "**********************************************\n",
      "Step :  360\n",
      "Iteration :  360\n",
      "Loss :  7.838565427436306\n",
      "**********************************************\n",
      "Step :  361\n",
      "Iteration :  361\n",
      "Loss :  7.838500062581915\n",
      "**********************************************\n",
      "Step :  362\n",
      "Iteration :  362\n",
      "Loss :  7.838794825289293\n",
      "**********************************************\n",
      "Step :  363\n",
      "Iteration :  363\n",
      "Loss :  7.838842818152774\n",
      "**********************************************\n",
      "Step :  364\n",
      "Iteration :  364\n",
      "Loss :  7.839033675516497\n",
      "**********************************************\n",
      "Step :  365\n",
      "Iteration :  365\n",
      "Loss :  7.839055241579991\n",
      "**********************************************\n",
      "Step :  366\n",
      "Iteration :  366\n",
      "Loss :  7.839131085727337\n",
      "**********************************************\n",
      "Step :  367\n",
      "Iteration :  367\n",
      "Loss :  7.838841716068173\n",
      "**********************************************\n",
      "Step :  368\n",
      "Iteration :  368\n",
      "Loss :  7.838671143154494\n",
      "**********************************************\n",
      "Step :  369\n",
      "Iteration :  369\n",
      "Loss :  7.838612289012234\n",
      "**********************************************\n",
      "Step :  370\n",
      "Iteration :  370\n",
      "Loss :  7.8386317647105574\n",
      "**********************************************\n",
      "Step :  371\n",
      "Iteration :  371\n",
      "Loss :  7.838804039018838\n",
      "**********************************************\n",
      "Step :  372\n",
      "Iteration :  372\n",
      "Loss :  7.838790847329001\n",
      "**********************************************\n",
      "Step :  373\n",
      "Iteration :  373\n",
      "Loss :  7.838672844742664\n",
      "**********************************************\n",
      "Step :  374\n",
      "Iteration :  374\n",
      "Loss :  7.838396851146399\n",
      "**********************************************\n",
      "Step :  375\n",
      "Iteration :  375\n",
      "Loss :  7.838527678153406\n",
      "**********************************************\n",
      "Step :  376\n",
      "Iteration :  376\n",
      "Loss :  7.838663224621852\n",
      "**********************************************\n",
      "Step :  377\n",
      "Iteration :  377\n",
      "Loss :  7.8387530002841155\n",
      "**********************************************\n",
      "Step :  378\n",
      "Iteration :  378\n",
      "Loss :  7.839030624014322\n",
      "**********************************************\n",
      "Step :  379\n",
      "Iteration :  379\n",
      "Loss :  7.839787760417022\n",
      "**********************************************\n",
      "Step :  380\n",
      "Iteration :  380\n",
      "Loss :  7.840583421177781\n",
      "**********************************************\n",
      "Step :  381\n",
      "Iteration :  381\n",
      "Loss :  7.841565389970407\n",
      "**********************************************\n",
      "Step :  382\n",
      "Iteration :  382\n",
      "Loss :  7.842823100098361\n",
      "**********************************************\n",
      "Step :  383\n",
      "Iteration :  383\n",
      "Loss :  7.844425464225693\n",
      "**********************************************\n",
      "Step :  384\n",
      "Iteration :  384\n",
      "Loss :  7.845963697657311\n",
      "**********************************************\n",
      "Step :  385\n",
      "Iteration :  385\n",
      "Loss :  7.847448869247833\n",
      "**********************************************\n",
      "Step :  386\n",
      "Iteration :  386\n",
      "Loss :  7.848741598201629\n",
      "**********************************************\n",
      "Step :  387\n",
      "Iteration :  387\n",
      "Loss :  7.848771525717703\n",
      "**********************************************\n",
      "Step :  388\n",
      "Iteration :  388\n",
      "Loss :  7.846686723565285\n",
      "**********************************************\n",
      "Step :  389\n",
      "Iteration :  389\n",
      "Loss :  7.842449022980189\n",
      "**********************************************\n",
      "Step :  390\n",
      "Iteration :  390\n",
      "Loss :  7.839068391276699\n",
      "**********************************************\n",
      "Step :  391\n",
      "Iteration :  391\n",
      "Loss :  7.839424336550194\n",
      "**********************************************\n",
      "Step :  392\n",
      "Iteration :  392\n",
      "Loss :  7.843067616650599\n",
      "**********************************************\n",
      "Step :  393\n",
      "Iteration :  393\n",
      "Loss :  7.8458908702828545\n",
      "**********************************************\n",
      "Step :  394\n",
      "Iteration :  394\n",
      "Loss :  7.845975602911903\n",
      "**********************************************\n",
      "Step :  395\n",
      "Iteration :  395\n",
      "Loss :  7.843095542647438\n",
      "**********************************************\n",
      "Step :  396\n",
      "Iteration :  396\n",
      "Loss :  7.839425112648309\n",
      "**********************************************\n",
      "Step :  397\n",
      "Iteration :  397\n",
      "Loss :  7.839285371579946\n",
      "**********************************************\n",
      "Step :  398\n",
      "Iteration :  398\n",
      "Loss :  7.8421291130922715\n",
      "**********************************************\n",
      "Step :  399\n",
      "Iteration :  399\n",
      "Loss :  7.843441309748091\n",
      "**********************************************\n",
      "Step :  400\n",
      "Iteration :  400\n",
      "Loss :  7.841811086881796\n",
      "**********************************************\n",
      "Step :  401\n",
      "Iteration :  401\n",
      "Loss :  7.839115252119537\n",
      "**********************************************\n",
      "Step :  402\n",
      "Iteration :  402\n",
      "Loss :  7.8394105646673475\n",
      "**********************************************\n",
      "Step :  403\n",
      "Iteration :  403\n",
      "Loss :  7.841694649125008\n",
      "**********************************************\n",
      "Step :  404\n",
      "Iteration :  404\n",
      "Loss :  7.841945461144739\n",
      "**********************************************\n",
      "Step :  405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration :  405\n",
      "Loss :  7.840038243042517\n",
      "**********************************************\n",
      "Step :  406\n",
      "Iteration :  406\n",
      "Loss :  7.838862663836238\n",
      "**********************************************\n",
      "Step :  407\n",
      "Iteration :  407\n",
      "Loss :  7.840983847292365\n",
      "**********************************************\n",
      "Step :  408\n",
      "Iteration :  408\n",
      "Loss :  7.843788714713951\n",
      "**********************************************\n",
      "Step :  409\n",
      "Iteration :  409\n",
      "Loss :  7.843321343299913\n",
      "**********************************************\n",
      "Step :  410\n",
      "Iteration :  410\n",
      "Loss :  7.840191608905611\n",
      "**********************************************\n",
      "Step :  411\n",
      "Iteration :  411\n",
      "Loss :  7.83888333690051\n",
      "**********************************************\n",
      "Step :  412\n",
      "Iteration :  412\n",
      "Loss :  7.841526145870601\n",
      "**********************************************\n",
      "Step :  413\n",
      "Iteration :  413\n",
      "Loss :  7.84396789296837\n",
      "**********************************************\n",
      "Step :  414\n",
      "Iteration :  414\n",
      "Loss :  7.842363687204074\n",
      "**********************************************\n",
      "Step :  415\n",
      "Iteration :  415\n",
      "Loss :  7.839185217504964\n",
      "**********************************************\n",
      "Step :  416\n",
      "Iteration :  416\n",
      "Loss :  7.839457447874812\n",
      "**********************************************\n",
      "Step :  417\n"
     ]
    }
   ],
   "source": [
    "save_pcl.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134dcde1",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_pose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mD:\\Projects\\ML3D\\Final Project\\ssl_3d_recon_pytorch\\src\\data\\shapenet.py:45\u001b[0m, in \u001b[0;36mShapeNet.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#rgb_image =  cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m  cv2\u001b[38;5;241m.\u001b[39mimread(img_path)\n\u001b[1;32m---> 45\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m transform(rgb_image)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# print(img_tensor.shape)\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "trainset[0]['random_pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976bc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[0]['random_pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20750b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
